{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8298724,"sourceType":"datasetVersion","datasetId":4929931},{"sourceId":8328622,"sourceType":"datasetVersion","datasetId":4945815},{"sourceId":8451018,"sourceType":"datasetVersion","datasetId":4945906}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fasttext-langdetect\n!pip install datasets\n!pip install stopwordsiso\n!pip install simplemma","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLO5rzkCSyhr","outputId":"bfe5ac1d-a6a3-4231-d7cb-16ed04d280be","executionInfo":{"status":"ok","timestamp":1714653748264,"user_tz":-120,"elapsed":21920,"user":{"displayName":"bombadon bombadillo","userId":"03041792117474631776"}},"execution":{"iopub.status.busy":"2024-05-18T14:00:06.501210Z","iopub.execute_input":"2024-05-18T14:00:06.501619Z","iopub.status.idle":"2024-05-18T14:01:00.866737Z","shell.execute_reply.started":"2024-05-18T14:00:06.501577Z","shell.execute_reply":"2024-05-18T14:01:00.865362Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fasttext-langdetect\n  Downloading fasttext-langdetect-1.0.5.tar.gz (6.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fasttext>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from fasttext-langdetect) (0.9.2)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from fasttext-langdetect) (2.31.0)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext>=0.9.1->fasttext-langdetect) (2.12.0)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext>=0.9.1->fasttext-langdetect) (69.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fasttext>=0.9.1->fasttext-langdetect) (1.26.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->fasttext-langdetect) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->fasttext-langdetect) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->fasttext-langdetect) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->fasttext-langdetect) (2024.2.2)\nBuilding wheels for collected packages: fasttext-langdetect\n  Building wheel for fasttext-langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fasttext-langdetect: filename=fasttext_langdetect-1.0.5-py3-none-any.whl size=7502 sha256=6439bafc872ee84e9a5af475185e5be85de8fa93ad195a4ce4571c4ac5284d8e\n  Stored in directory: /root/.cache/pip/wheels/e6/5b/5d/47e2fd5c2ff1028722739ce35f365e8f6eeb89ec97aa63e621\nSuccessfully built fasttext-langdetect\nInstalling collected packages: fasttext-langdetect\nSuccessfully installed fasttext-langdetect-1.0.5\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting stopwordsiso\n  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\nDownloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: stopwordsiso\nSuccessfully installed stopwordsiso-0.6.1\nCollecting simplemma\n  Downloading simplemma-0.9.1-py3-none-any.whl.metadata (19 kB)\nDownloading simplemma-0.9.1-py3-none-any.whl (75.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: simplemma\nSuccessfully installed simplemma-0.9.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"q4fPShGPS9Pr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714653680265,"user_tz":-120,"elapsed":22431,"user":{"displayName":"bombadon bombadillo","userId":"03041792117474631776"}},"outputId":"60b6996f-949c-4283-e8c3-1638f7731a99"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"code","source":"import os\ncount = 0\nfor root, folders, filenames in os.walk('/kaggle/input'):\n   print(root, folders)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:00.870486Z","iopub.execute_input":"2024-05-18T14:01:00.870898Z","iopub.status.idle":"2024-05-18T14:01:00.891702Z","shell.execute_reply.started":"2024-05-18T14:01:00.870865Z","shell.execute_reply":"2024-05-18T14:01:00.890623Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input ['autext', 'autext2024', 'models']\n/kaggle/input/autext []\n/kaggle/input/autext2024 []\n/kaggle/input/models []\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset_en = Dataset.from_json(\"/kaggle/input/autext/train.jsonl\")\n\ndataset_en","metadata":{"id":"zE7O0AitS8-m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714653759662,"user_tz":-120,"elapsed":3,"user":{"displayName":"bombadon bombadillo","userId":"03041792117474631776"}},"outputId":"e79793aa-5db0-4444-8620-f076c8449981","execution":{"iopub.status.busy":"2024-05-18T14:01:00.892717Z","iopub.execute_input":"2024-05-18T14:01:00.893078Z","iopub.status.idle":"2024-05-18T14:01:04.588237Z","shell.execute_reply.started":"2024-05-18T14:01:00.893040Z","shell.execute_reply":"2024-05-18T14:01:04.587387Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e51a4ec82a041f29f6c92a928c22b5a"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'text', 'label'],\n    num_rows: 109663\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_en['label'][0:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:04.592626Z","iopub.execute_input":"2024-05-18T14:01:04.593142Z","iopub.status.idle":"2024-05-18T14:01:04.720745Z","shell.execute_reply.started":"2024-05-18T14:01:04.593111Z","shell.execute_reply":"2024-05-18T14:01:04.719527Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['generated',\n 'human',\n 'generated',\n 'generated',\n 'generated',\n 'generated',\n 'generated',\n 'human',\n 'human',\n 'human']"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Assuming you have dataset_en already loaded and containing the 'train' split\n\n# Define a mapping from original labels to new labels\nlabel_mapping = {\"human\": 0, \"generated\": 1}\n\n# Function to replace labels\ndef map_labels(example):\n    # Map the original label to the new label\n    example[\"label\"] = label_mapping[example[\"label\"]]\n    return example\n\n# Map the labels in the 'train' split\ndataset_en = dataset_en.map(map_labels)\n\ncustom_label_names = [\"human\", \"generated\"]\n\ndataset_en['label'][0:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:04.722072Z","iopub.execute_input":"2024-05-18T14:01:04.722391Z","iopub.status.idle":"2024-05-18T14:01:12.065353Z","shell.execute_reply.started":"2024-05-18T14:01:04.722367Z","shell.execute_reply":"2024-05-18T14:01:12.064290Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/109663 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58829bbb0ab74dbdb552dad4cccb063a"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[1, 0, 1, 1, 1, 1, 1, 0, 0, 0]"},"metadata":{}}]},{"cell_type":"code","source":"from ftlangdetect import detect\nimport re\n\n# Initialize an empty dictionary to store the texts for each language\nlanguage_datasets = {}\nlanguages = [\"en\", \"es\", \"ca\", \"pt\", \"eu\", \"gl\"]\n\n# Assuming dataset_en is a list of texts\nfor text,label in zip(dataset_en['text'], dataset_en['label']):\n    try:\n        \n        language = detect(text)\n    except:\n        language = \"Error, could not detect\"\n\n    # If the language is not already a key in the dictionary, add it with an empty list as its value\n    if language['lang'] not in languages:\n        continue\n        if language['lang'] == 'it':\n            language['lang'] = 'pt'\n        elif language['lang'] == 'fr':\n            language['lang'] = 'ca'\n        else:\n            language['lang'] = 'es'\n    \n    if language['lang'] not in language_datasets:\n        language_datasets[language['lang']] = []\n\n    # Append the text to the list of texts for its detected language\n    language_datasets[language['lang']].append((text, label))","metadata":{"id":"TZmaguJSVBDj","executionInfo":{"status":"ok","timestamp":1714654005151,"user_tz":-120,"elapsed":28311,"user":{"displayName":"bombadon bombadillo","userId":"03041792117474631776"}},"execution":{"iopub.status.busy":"2024-05-18T14:01:12.066767Z","iopub.execute_input":"2024-05-18T14:01:12.067729Z","iopub.status.idle":"2024-05-18T14:01:37.625025Z","shell.execute_reply.started":"2024-05-18T14:01:12.067695Z","shell.execute_reply":"2024-05-18T14:01:37.624145Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"}]},{"cell_type":"code","source":"language_datasets.keys()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:37.626113Z","iopub.execute_input":"2024-05-18T14:01:37.626407Z","iopub.status.idle":"2024-05-18T14:01:37.632154Z","shell.execute_reply.started":"2024-05-18T14:01:37.626383Z","shell.execute_reply":"2024-05-18T14:01:37.631171Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dict_keys(['ca', 'es', 'gl', 'en', 'pt', 'eu'])"},"metadata":{}}]},{"cell_type":"code","source":"# Set your threshold\nthreshold = 1000\n\n# Create a new dictionary with only the items where the length of the list is greater than or equal to the threshold\n#language_datasets = {language: texts for language, texts in language_datasets.items() if len(texts) >= threshold}\n\n# Assuming language_datasets is your dictionary of languages and texts\nfor language, texts in language_datasets.items():\n    print(f\"Language: {language}, Number of texts: {len(texts)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIA344iTVfHG","executionInfo":{"status":"ok","timestamp":1714654114679,"user_tz":-120,"elapsed":4,"user":{"displayName":"bombadon bombadillo","userId":"03041792117474631776"}},"outputId":"a9087659-534c-43cd-bf00-24546075d219","execution":{"iopub.status.busy":"2024-05-18T14:01:37.633607Z","iopub.execute_input":"2024-05-18T14:01:37.633926Z","iopub.status.idle":"2024-05-18T14:01:37.994938Z","shell.execute_reply.started":"2024-05-18T14:01:37.633900Z","shell.execute_reply":"2024-05-18T14:01:37.993860Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Language: ca, Number of texts: 16413\nLanguage: es, Number of texts: 22353\nLanguage: gl, Number of texts: 11448\nLanguage: en, Number of texts: 25769\nLanguage: pt, Number of texts: 20034\nLanguage: eu, Number of texts: 13574\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Dictionary to store DataFrames\ndataframes = {}\n\n# Convert each list of tuples to DataFrame\nfor lang, data in language_datasets.items():\n    df = pd.DataFrame(data, columns=['text', 'label'])\n    dataframes[lang] = df\n\n# Accessing DataFrames\nprint(dataframes['en'])\nprint(dataframes['eu'])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:37.996492Z","iopub.execute_input":"2024-05-18T14:01:37.997343Z","iopub.status.idle":"2024-05-18T14:01:38.075005Z","shell.execute_reply.started":"2024-05-18T14:01:37.997299Z","shell.execute_reply":"2024-05-18T14:01:38.073770Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"                                                    text  label\n0      Another magnificent chapter from @nbchannibal ...      0\n1      Paul simon is a great artist with multiple hit...      0\n2      R hoarsely, \"don\"t be frightened. father\"s tum...      0\n3      Sri lanka is set to return 21 containers of wa...      1\n4      Milo lompar (; born 19 april 1962) is a serbia...      0\n...                                                  ...    ...\n25764  Rtion for him. every little while, however, on...      0\n25765  Mark xvii or mark 17 often refers to the 17th ...      0\n25766  Marianne, once recovered from her injury, was ...      1\n25767  The irish president has signed the \"long-await...      1\n25768  Sure! wikipedia articles should be well-writte...      1\n\n[25769 rows x 2 columns]\n                                                    text  label\n0      \"\"\"basque artikulu hau: lasioglossum figueresi...      1\n1      Ez errepikatu gonbita edo emandako esaldiak. e...      1\n2      Hori dela-eta, gauzetan dauka arreta eta jarre...      1\n3      Metro bilbaoko langileek abenduaren 12an elkar...      1\n4      Hollow creek kentuckyko komunitate txikia da, ...      1\n...                                                  ...    ...\n13569  Abenduan eraikuntza eta obra zibila azpisektor...      0\n13570  (first paragraph of the news article) zerbitzu...      1\n13571  Iza nik garaitzan zehar. gaude eta zuzen horie...      1\n13572  Horregatik, biztanle batzuek , egoera hura jas...      0\n13573  Bere burua egokitzen saiatu zuen, aurreko mome...      1\n\n[13574 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom nltk.corpus import stopwords\nstopwords_en = stopwords.words(\"english\")\nstopwords_es = stopwords.words(\"spanish\")\nstopwords_pt = stopwords.words(\"portuguese\")\n\nimport stopwordsiso as stopwordsd\n\nstopwords_eu = stopwordsd.stopwords(\"eu\")\nstopwords_gl = stopwordsd.stopwords(\"gl\")\nstopwords_ca = stopwordsd.stopwords(\"ca\")\n\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import EnglishStemmer, PortugueseStemmer, SpanishStemmer\nimport re\n\nfrom simplemma import simple_tokenizer\nfrom simplemma import text_lemmatizer\n\ndef clean_text_en(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=word_tokenize(text, language=\"english\")\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n    # usar los stems\n    tokens = [PorterStemmer().stem(word) for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token)>=min_length and p.match(token):\n            filtered_tokens.append(token)\n\n    return filtered_tokens\n\ndef clean_text_es(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=word_tokenize(text, language=\"spanish\")\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwords.words(\"spanish\")]\n    # usar los stems\n    tokens = [SpanishStemmer().stem(word) for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token)>=min_length and p.match(token):\n            filtered_tokens.append(token)\n\n    return filtered_tokens\n\ndef clean_text_pt(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=word_tokenize(text, language=\"portuguese\")\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwords.words(\"portuguese\")]\n    # usar los stems\n    tokens = [PortugueseStemmer().stem(word) for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token)>=min_length and p.match(token):\n            filtered_tokens.append(token)\n\n    return filtered_tokens\n\ndef clean_text_gl(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=simple_tokenizer(text)\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwordsd.stopwords(\"gl\")]\n    # usar los stems\n    tokens = [text_lemmatizer(word, lang='gl') for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token[0])>=min_length and p.match(token[0]):\n            filtered_tokens.append(token[0])\n\n    return filtered_tokens\n\ndef clean_text_eu(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=simple_tokenizer(text)\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwordsd.stopwords(\"eu\")]\n    # usar los stems\n    tokens = [text_lemmatizer(word, lang='eu') for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token[0])>=min_length and p.match(token[0]):\n            filtered_tokens.append(token[0])\n\n    return filtered_tokens\n\ndef clean_text_ca(text):\n    # transformar a minúscula\n    text=str(text).lower()\n    # tokenizar\n    tokens=simple_tokenizer(text)\n    # borrar stopwords\n    tokens = [word for word in tokens if word not in stopwordsd.stopwords(\"ca\")]\n    # usar los stems\n    tokens = [text_lemmatizer(word, lang='ca') for word in tokens]\n    # eliminamos las palabras con menos de 3 caráceres\n    min_length = 3\n    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n    p = re.compile('^[a-zA-Z]+$');\n\n    filtered_tokens=[]\n    for token in tokens:\n        if len(token[0])>=min_length and p.match(token[0]):\n            filtered_tokens.append(token[0])\n\n    return filtered_tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:38.079377Z","iopub.execute_input":"2024-05-18T14:01:38.079719Z","iopub.status.idle":"2024-05-18T14:01:38.370935Z","shell.execute_reply.started":"2024-05-18T14:01:38.079693Z","shell.execute_reply":"2024-05-18T14:01:38.369949Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"clean_text_eu(dataframes['eu']['text'][0])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:38.372460Z","iopub.execute_input":"2024-05-18T14:01:38.372742Z","iopub.status.idle":"2024-05-18T14:01:38.381373Z","shell.execute_reply.started":"2024-05-18T14:01:38.372718Z","shell.execute_reply":"2024-05-18T14:01:38.380276Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['basque',\n 'artikulu',\n 'lasioglossum',\n 'figueresi',\n 'lasioglossum',\n 'buruzko',\n 'lekuz',\n 'aldetik',\n 'mahaigaindiko',\n 'blokean',\n 'iparraldeko',\n 'lurralde',\n 'lurraldeetan',\n 'lehia',\n 'lardo',\n 'zizkaidoa',\n 'eman',\n 'lasioglossum',\n 'megachilenoak',\n 'gainako',\n 'ibaiako']"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport joblib\nfrom sklearn.metrics import classification_report, f1_score\n\nlanguages = [\"en\", \"es\", \"ca\", \"pt\", \"eu\", \"gl\"]\n\nclean_text_functions = {\n    \"en\": clean_text_en,\n    \"es\": clean_text_es,\n    \"ca\": clean_text_ca,\n    \"pt\": clean_text_pt,\n    \"eu\": clean_text_eu,\n    \"gl\": clean_text_gl,\n}\n\nfor lang, dataset in dataframes.items():\n    print(f\"\\nlanguage: {lang}\\n\")\n    # train test splits\n    if lang in [\"ca\", \"es\", \"gl\"]:#, \"pt\", \"en\"]:\n        continue\n        \n    #train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n    train_df = dataset\n    print(\"Train set shape:\", train_df.shape)\n    #print(\"Test set shape:\", test_df.shape)\n    \n    # Transformar labels a numéricas\n    y_train = train_df['label'].tolist()\n    \n    #y_test = test_df['label'].tolist()\n    \n    # Pipeline con SVC \n    pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_text_functions[lang])),\n    ('tf', TfidfTransformer()),\n    ('svm', SVC()),\n    ])\n\n    X_train = train_df['text'].tolist()\n\n    # entrenamos el pipeline\n    pipeline.fit(X_train, y_train)\n\n    # Guardamos el modelo\n    joblib.dump(pipeline, f'full_model_{lang}.pkl')\n      \n    \"\"\"\n    # Predicciones sobre test y resultados\n    X_test= test_df['text'].tolist()\n    predictions = pipeline.predict(X_test)\n    print(classification_report(y_test, predictions, target_names=LABELS))\n    print(f\"F1 Score: {f1_score(y_test, predictions, average='macro')}\")\n    \"\"\"\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:01:53.294829Z","iopub.execute_input":"2024-05-18T14:01:53.295329Z","iopub.status.idle":"2024-05-18T15:00:54.944663Z","shell.execute_reply.started":"2024-05-18T14:01:53.295289Z","shell.execute_reply":"2024-05-18T15:00:54.943224Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-05-18 14:01:55.456922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-18 14:01:55.457078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-18 14:01:55.613650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nlanguage: ca\n\n\nlanguage: es\n\n\nlanguage: gl\n\n\nlanguage: en\n\nTrain set shape: (25769, 2)\n\nlanguage: pt\n\nTrain set shape: (20034, 2)\n\nlanguage: eu\n\nTrain set shape: (13574, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset_test = Dataset.from_json(\"/kaggle/input/autext/test.jsonl\")\n\ndataset_test","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:02:41.910877Z","iopub.execute_input":"2024-05-18T15:02:41.912118Z","iopub.status.idle":"2024-05-18T15:02:42.534262Z","shell.execute_reply.started":"2024-05-18T15:02:41.912063Z","shell.execute_reply":"2024-05-18T15:02:42.533024Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f3f4486a634cb9b35072ca81971ae2"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'text'],\n    num_rows: 43365\n})"},"metadata":{}}]},{"cell_type":"code","source":"from ftlangdetect import detect\nimport re\n\n# Initialize an empty dictionary to store the texts for each language\nlanguage_datasets = {}\nlanguages = [\"en\", \"es\", \"ca\", \"pt\", \"eu\", \"gl\"]\n\n# Assuming dataset_en is a list of texts\nfor id_t,text in zip(dataset_test['id'], dataset_test['text']):\n    try:\n        language = detect(text)\n    except:\n        language = \"Error, could not detect\"\n\n    # If the language is not already a key in the dictionary, add it with an empty list as its value\n    if language['lang'] not in languages:\n        if language['lang'] == 'it':\n            language['lang'] = 'pt'\n        elif language['lang'] == 'fr':\n            language['lang'] = 'ca'\n        else:\n            language['lang'] = 'es'\n\n    if language['lang'] not in language_datasets:\n        language_datasets[language['lang']] = []\n\n    # Append the text to the list of texts for its detected language\n    language_datasets[language['lang']].append((id_t,text))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:02:44.711811Z","iopub.execute_input":"2024-05-18T15:02:44.712205Z","iopub.status.idle":"2024-05-18T15:02:50.264570Z","shell.execute_reply.started":"2024-05-18T15:02:44.712177Z","shell.execute_reply":"2024-05-18T15:02:50.263348Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Set your threshold\nthreshold = 1000\n\n# Create a new dictionary with only the items where the length of the list is greater than or equal to the threshold\n#language_datasets = {language: texts for language, texts in language_datasets.items() if len(texts) >= threshold}\n\n# Assuming language_datasets is your dictionary of languages and texts\nfor language, texts in language_datasets.items():\n    print(f\"Language: {language}, Number of texts: {len(texts)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:02:50.266205Z","iopub.execute_input":"2024-05-18T15:02:50.266525Z","iopub.status.idle":"2024-05-18T15:02:50.272822Z","shell.execute_reply.started":"2024-05-18T15:02:50.266499Z","shell.execute_reply":"2024-05-18T15:02:50.271611Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Language: en, Number of texts: 10324\nLanguage: ca, Number of texts: 8013\nLanguage: pt, Number of texts: 9003\nLanguage: es, Number of texts: 9116\nLanguage: eu, Number of texts: 5518\nLanguage: gl, Number of texts: 1391\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Dictionary to store DataFrames\ndataframes = {}\n\nlabel_mapping = {\"human\": 0, \"generated\": 1}\n\ndef map_labels(example):\n        # Map the original label to the new label\n        example[\"label\"] = label_mapping[example[\"label\"]]\n        return example\n\n# Convert each list of tuples to DataFrame\nfor lang, data in language_datasets.items():\n    df = pd.DataFrame(data, columns=['id', 'text'])\n\n    dataframes[lang] = Dataset.from_pandas(df)\n\ndataframes","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:02:50.274293Z","iopub.execute_input":"2024-05-18T15:02:50.274918Z","iopub.status.idle":"2024-05-18T15:02:50.484589Z","shell.execute_reply.started":"2024-05-18T15:02:50.274879Z","shell.execute_reply":"2024-05-18T15:02:50.483782Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'en': Dataset({\n     features: ['id', 'text'],\n     num_rows: 10324\n }),\n 'ca': Dataset({\n     features: ['id', 'text'],\n     num_rows: 8013\n }),\n 'pt': Dataset({\n     features: ['id', 'text'],\n     num_rows: 9003\n }),\n 'es': Dataset({\n     features: ['id', 'text'],\n     num_rows: 9116\n }),\n 'eu': Dataset({\n     features: ['id', 'text'],\n     num_rows: 5518\n }),\n 'gl': Dataset({\n     features: ['id', 'text'],\n     num_rows: 1391\n })}"},"metadata":{}}]},{"cell_type":"code","source":"import joblib\n\nlanguages = [\"en\", \"es\", \"ca\", \"pt\", \"eu\", \"gl\"]\n\nmodels_svm_lang = {}\n\nfor lang in languages:\n\n    models_svm_lang[lang] = joblib.load(f\"/kaggle/input/models/full_model_{lang}.pkl\")\n\nmodels_svm_lang","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:02:50.486297Z","iopub.execute_input":"2024-05-18T15:02:50.486781Z","iopub.status.idle":"2024-05-18T15:02:53.108653Z","shell.execute_reply.started":"2024-05-18T15:02:50.486753Z","shell.execute_reply":"2024-05-18T15:02:53.107288Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'en': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_en at 0x7d153f862dd0>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())]),\n 'es': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_es at 0x7d14b80f6830>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())]),\n 'ca': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_ca at 0x7d14b80f6a70>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())]),\n 'pt': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_pt at 0x7d151aa5e8c0>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())]),\n 'eu': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_eu at 0x7d151aa5e9e0>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())]),\n 'gl': Pipeline(steps=[('bow',\n                  CountVectorizer(analyzer=<function clean_text_gl at 0x7d152009add0>)),\n                 ('tf', TfidfTransformer()), ('svm', SVC())])}"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n\nprefix = 'svm'\n\nfor lang in languages:\n\n    print(lang)\n    X_test = dataframes[lang]['text']\n    X_ids = dataframes[lang]['id']\n\n    y_preds = models_svm_lang[lang].predict(X_test)\n\n    svm_preds = [(id_t, pred) for id_t,pred in zip(X_ids, y_preds)]\n\n    with open(f\"/kaggle/working/{prefix}_test_{lang}.pkl\", \"wb\") as f:\n        pickle.dump(svm_preds, f)\n\n\n\nsvm_preds[0:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T15:03:01.328435Z","iopub.execute_input":"2024-05-18T15:03:01.328828Z","iopub.status.idle":"2024-05-18T15:19:25.351948Z","shell.execute_reply.started":"2024-05-18T15:03:01.328800Z","shell.execute_reply":"2024-05-18T15:19:25.351086Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"en\nes\nca\npt\neu\ngl\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[('151315', 1),\n ('25491', 1),\n ('91451', 1),\n ('88976', 1),\n ('105491', 0),\n ('80721', 1),\n ('109998', 1),\n ('93086', 1),\n ('150366', 1),\n ('7212', 1)]"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndataframe_en = dataframes['en']  \ndataframe_en = dataframes['es']  \ndataframe_en = dataframes['gl']  \n\n# Perform train-test split\ntrain_df_en, test_df_en = train_test_split(dataframe_en, test_size=0.2, random_state=42)\n\n# Output the shapes of train and test sets\nprint(\"Train set shape:\", train_df_en.shape)\nprint(\"Test set shape:\", test_df_en.shape)\ntrain_df_en","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:00:42.675768Z","iopub.execute_input":"2024-05-03T10:00:42.676211Z","iopub.status.idle":"2024-05-03T10:00:42.697425Z","shell.execute_reply.started":"2024-05-03T10:00:42.676176Z","shell.execute_reply":"2024-05-03T10:00:42.696138Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Train set shape: (9158, 2)\nTest set shape: (2290, 2)\n","output_type":"stream"},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"                                                    text      label\n2018   Vilela, lamas, san sadurniño é un concello sit...  generated\n9654   Non podo evitar sentir unha certa decepción po...  generated\n11361  Con esta iniciativa queriamos convidar os rapa...      human\n7800   A fonsagrada, 1951) tiña claro que a súa vocac...  generated\n3353   Delle é un lugar da parroquia de ferreiros no ...      human\n...                                                  ...        ...\n11284  Pola súa parte, nerea lage e marcos burgo eran...      human\n5191   Prudencia santasmarinas raposo, nada en 1943 e...      human\n5390   O escorial, recaré, o valadouro é unha localid...  generated\n860    A navalla de occam, ou principio da parsimonia...      human\n7270   O sukhoi su-24 (en rus: сухой су-24; en nomenc...  generated\n\n[9158 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018</th>\n      <td>Vilela, lamas, san sadurniño é un concello sit...</td>\n      <td>generated</td>\n    </tr>\n    <tr>\n      <th>9654</th>\n      <td>Non podo evitar sentir unha certa decepción po...</td>\n      <td>generated</td>\n    </tr>\n    <tr>\n      <th>11361</th>\n      <td>Con esta iniciativa queriamos convidar os rapa...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>7800</th>\n      <td>A fonsagrada, 1951) tiña claro que a súa vocac...</td>\n      <td>generated</td>\n    </tr>\n    <tr>\n      <th>3353</th>\n      <td>Delle é un lugar da parroquia de ferreiros no ...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11284</th>\n      <td>Pola súa parte, nerea lage e marcos burgo eran...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>5191</th>\n      <td>Prudencia santasmarinas raposo, nada en 1943 e...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>O escorial, recaré, o valadouro é unha localid...</td>\n      <td>generated</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>A navalla de occam, ou principio da parsimonia...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>7270</th>\n      <td>O sukhoi su-24 (en rus: сухой су-24; en nomenc...</td>\n      <td>generated</td>\n    </tr>\n  </tbody>\n</table>\n<p>9158 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\ny_train = train_df_en['label'].tolist()\ny_test = test_df_en['label'].tolist()\n\nle = LabelEncoder()\n\nprint(\"antes de transform: \", y_train[:10])\ny_train = le.fit_transform(y_train)\nLABELS = le.classes_\nNUM_LABELS = len(LABELS)\nprint(\"después de transform:\", y_train[:10])\n\nprint(\"antes de transform: \", y_test[:10])\ny_test = le.transform(y_test)\nprint(\"después de transform:\", y_test[:10])\n\nidx2label={}\nlabel2idx={}\nfor index, label in enumerate(LABELS):\n    label2idx.update([(label, index)])\n    idx2label.update([(index, label)])\n#y_train_cat = to_categorical(y_train)\n#y_test_cat = to_categorical(y_test)\nprint('Labels:', label2idx)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:00:50.335144Z","iopub.execute_input":"2024-05-03T10:00:50.335572Z","iopub.status.idle":"2024-05-03T10:00:50.360294Z","shell.execute_reply.started":"2024-05-03T10:00:50.335540Z","shell.execute_reply":"2024-05-03T10:00:50.358791Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"antes de transform:  ['generated', 'generated', 'human', 'generated', 'human', 'human', 'human', 'human', 'human', 'generated']\ndespués de transform: [0 0 1 0 1 1 1 1 1 0]\nantes de transform:  ['human', 'human', 'generated', 'human', 'generated', 'human', 'human', 'human', 'human', 'human']\ndespués de transform: [1 1 0 1 0 1 1 1 1 1]\nLabels: {'generated': 0, 'human': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n\n# definimos un pipeline que primero transforma los textos a BoW\n# después los textos se transforma en formato tfidf\n# finalmente, se aplica el algoritmo\npipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_text)),\n    ('tf', TfidfTransformer()),\n    ('svm', SVC()),\n])\n\n# totamos los textos\nX_train = train_df_en['text'].tolist()\n\n# entrenamos el pipeline\npipeline.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:00:53.139591Z","iopub.execute_input":"2024-05-03T10:00:53.140071Z","iopub.status.idle":"2024-05-03T10:02:08.542625Z","shell.execute_reply.started":"2024-05-03T10:00:53.140034Z","shell.execute_reply":"2024-05-03T10:02:08.541690Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('bow',\n                 CountVectorizer(analyzer=<function clean_text at 0x7fe0f07eecb0>)),\n                ('tf', TfidfTransformer()), ('svm', SVC())])","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n                 CountVectorizer(analyzer=&lt;function clean_text at 0x7fe0f07eecb0&gt;)),\n                (&#x27;tf&#x27;, TfidfTransformer()), (&#x27;svm&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n                 CountVectorizer(analyzer=&lt;function clean_text at 0x7fe0f07eecb0&gt;)),\n                (&#x27;tf&#x27;, TfidfTransformer()), (&#x27;svm&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function clean_text at 0x7fe0f07eecb0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"import joblib\njoblib.dump(pipeline, 'model_gl.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:09:01.355236Z","iopub.execute_input":"2024-05-03T10:09:01.355625Z","iopub.status.idle":"2024-05-03T10:09:01.964411Z","shell.execute_reply.started":"2024-05-03T10:09:01.355596Z","shell.execute_reply":"2024-05-03T10:09:01.963242Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"['model_gl.pkl']"},"metadata":{}}]},{"cell_type":"code","source":"# totamos los textos del conjunto test\nX_test= test_df_en['text'].tolist()\n# las labels ya fueron cargadas y transformadas en el apartado de Label encoding.\n\n# usamos el modelo para inferir las predicciones\npredictions = pipeline.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:02:13.128196Z","iopub.execute_input":"2024-05-03T10:02:13.128561Z","iopub.status.idle":"2024-05-03T10:02:26.803457Z","shell.execute_reply.started":"2024-05-03T10:02:13.128535Z","shell.execute_reply":"2024-05-03T10:02:26.802020Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint( classification_report(y_test, predictions, target_names=LABELS))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:02:26.805210Z","iopub.execute_input":"2024-05-03T10:02:26.805545Z","iopub.status.idle":"2024-05-03T10:02:26.827782Z","shell.execute_reply.started":"2024-05-03T10:02:26.805517Z","shell.execute_reply":"2024-05-03T10:02:26.826402Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n   generated       0.87      0.86      0.87      1234\n       human       0.84      0.86      0.85      1056\n\n    accuracy                           0.86      2290\n   macro avg       0.86      0.86      0.86      2290\nweighted avg       0.86      0.86      0.86      2290\n\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test= test_df_en['text'].tolist()\nclean_text(X_test[0]), X_test[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:00:23.912743Z","iopub.execute_input":"2024-05-03T10:00:23.913174Z","iopub.status.idle":"2024-05-03T10:00:23.927595Z","shell.execute_reply.started":"2024-05-03T10:00:23.913142Z","shell.execute_reply":"2024-05-03T10:00:23.926134Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"hey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nhey\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(['oviedo',\n  'rugby',\n  'club',\n  'entidade',\n  'deportivo',\n  'cidade',\n  'oviedo',\n  'dedicar',\n  'rugby',\n  'actualmente',\n  'militar',\n  'honrar',\n  'segundo',\n  'rugby',\n  'historiar',\n  'oviedo',\n  'fundar',\n  'ano',\n  'comer',\n  'resultar',\n  'dous',\n  'club',\n  'vello',\n  'cau',\n  'oviedo',\n  'economicas',\n  'dous',\n  'club',\n  'habitual',\n  'primeiro',\n  'nacional',\n  'daquela',\n  'segundo',\n  'rugby',\n  'nacional',\n  'tempada',\n  'novo',\n  'equipar',\n  'ascender',\n  'primeiro',\n  'nacional',\n  'acadar',\n  'caber',\n  'salientar',\n  'dende',\n  'ano',\n  'atar',\n  'actualidade',\n  'oviedo',\n  'levar',\n  'militar',\n  'sempre',\n  'nalgunha',\n  'rugby',\n  'seguinte',\n  'tornear',\n  'equipar',\n  'rematar',\n  'cal',\n  'ascender',\n  'honrar',\n  'debutar',\n  'tempada',\n  'nesa',\n  'primeiro',\n  'elite',\n  'rugby',\n  'oviedo',\n  'rematar',\n  'cal',\n  'descender',\n  'segundo',\n  'metade',\n  'oviedo',\n  'rematar',\n  'primeiro',\n  'tres',\n  'vez',\n  'grupo',\n  'primeiro',\n  'nacional',\n  'vez',\n  'acadar',\n  'ascenso',\n  'tempada',\n  'honrar',\n  'votar',\n  'ano',\n  'mellor',\n  'resultar',\n  'obter',\n  'tempada',\n  'seguinte',\n  'ano',\n  'equipar',\n  'descender',\n  'honrar',\n  'militar',\n  'dende',\n  'tempada',\n  'oviedo',\n  'rematar',\n  'grupo',\n  'honrar',\n  'oviedo',\n  'ligueiras',\n  'dende',\n  'ano',\n  'honrar',\n  'existir',\n  'dende',\n  'tempada',\n  'anteriormente',\n  'primeiro',\n  'nacional',\n  'rugby',\n  'outro',\n  'artigo',\n  'ligar'],\n 'O oviedo rugby club é unha entidade deportiva da cidade de oviedo que se dedica ó rugby e que actualmente milita na división de honra b, a segunda categoría do rugby en españa. historia o oviedo rc foi fundado no ano 1983 como resultado da fusión de dous clubs mais vellos, o cau oviedo e o economicas. ámbolos dous clubs eran habituais da primeira división nacional, que daquela era a segunda división do rugby nacional. na tempada 1986-87 o novo equipo ascendeu á primeira nacional, e acadou un 4º posto. cabe salientar que dende ese ano 1987 ata a actualidade o oviedo rc leva 24 militando sempre nalgunha das 2 categorías máximas do rugby en españa. nas seguintes edicións do torneo o equipo rematou 2º e 1º, polo cal ascendeu á división de honra, na que debutou na tempada 1990-91. nesa primeira participación na elite do rugby español, o oviedo rc rematou 12º e último, co cal descendeu. na segunda metade da década dos 90 o oviedo rc rematou primeiro tres veces no seu grupo da primeira nacional, e unha vez 2º, acadando o ascenso na tempada 1997-98. en división de honra votou 3 anos. o seu mellor resultado foi a 7ª posición obtida na tempada 1999-2000. ó seguinte ano o equipo descendeu a división de honra b, na que milita dende entón. na tempada 2010-11 o oviedo rc rematou 5º no grupo a da división de honra b. posicións do oviedo rc en competicións ligueiras dende o ano da súa fundación a división de honra b existe dende a tempada 1998-99. anteriormente a primeira nacional era a 2ª división do rugby español. véxase tamén outros artigos liga')"},"metadata":{}}]},{"cell_type":"code","source":"import fasttext.util\nfasttext.util.download_model('gl', if_exists='ignore')\nft = fasttext.load_model('cc.gl.300.bin')","metadata":{},"execution_count":null,"outputs":[]}]}